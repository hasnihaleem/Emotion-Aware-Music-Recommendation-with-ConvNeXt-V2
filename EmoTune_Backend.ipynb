{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2pvY1wJDSwO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4013b3b0-2035-483d-af98-5a30aca545ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install flask flask-cors torch torchvision timm numpy opencv-python-headless pyngrok -q\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import timm\n",
        "import numpy as np\n",
        "import cv2\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok, conf\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a7EIzqS9TFk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9271f82-2c9d-4454-8b75-1466df85590b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<flask_cors.extension.CORS at 0x7a7e683bea10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "NGROK_AUTH_TOKEN = '2uPLcmFMRXK46DMBToujQlsoMcj_3fPsn9JzGNaFJfLrUPw1z'\n",
        "MODEL_PATH = '/content/drive/MyDrive/model/best_model.pth'\n",
        "drive.mount('/content/drive')\n",
        "MODEL_ARCHITECTURE = \"convnextv2_tiny.fcmae_ft_in1k\"\n",
        "NUM_CLASSES = 5\n",
        "CLASS_NAMES = ['angry', 'happy', 'neutral', 'sad', 'surprise']\n",
        "\n",
        "IM_SIZE = 224\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device.type}\")\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "CORS(app)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cdBpZv8JTUiS",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98cf5f78-5cc9-4db3-89f6-e7879ea539de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emotion detection model loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "model = None\n",
        "try:\n",
        "    model = timm.create_model(MODEL_ARCHITECTURE, pretrained=False, num_classes=NUM_CLASSES)\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(\"Emotion detection model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load model. Check the MODEL_PATH. Error: {e}\")\n",
        "\n",
        "preprocess_transform = transforms.Compose([\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RQbH3Rh_a0zf"
      },
      "outputs": [],
      "source": [
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return \"EmoTune PyTorch Backend API is running!\"\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "\n",
        "    if not model:\n",
        "        return jsonify({\"error\": \"Model is not loaded or failed to load.\"}), 500\n",
        "\n",
        "    if 'image' not in request.files:\n",
        "        return jsonify({\"error\": \"No image file provided in the request.\"}), 400\n",
        "\n",
        "    image_file = request.files['image']\n",
        "\n",
        "    try:\n",
        "\n",
        "        img = Image.open(image_file.stream).convert(\"RGB\")\n",
        "\n",
        "        input_tensor = preprocess_transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            confidence, predicted_index = torch.max(probabilities, 1)\n",
        "            predicted_emotion = CLASS_NAMES[predicted_index.item()]\n",
        "\n",
        "        return jsonify({\n",
        "            \"emotion\": predicted_emotion,\n",
        "            \"confidence\": confidence.item()\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Prediction failed: {str(e)}\")\n",
        "        return jsonify({\"error\": f\"An error occurred during prediction: {str(e)}\"}), 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAfGTRq7a6Rc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8c266d-ca09-45f2-f7b2-14b47dae6bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Flask server with ngrok...\n",
            "Backend is running and accessible at: NgrokTunnel: \"https://e640-34-16-156-94.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:38:16] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:38:22] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:38:30] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:38:36] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:38:48] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:38:56] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:39:04] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:39:11] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:39:20] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:39:26] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:39:39] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:40:07] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:40:20] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:42:25] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:42:43] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:43:02] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:43:27] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:43:35] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:44:05] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 09:44:22] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Flask server with ngrok...\")\n",
        "    try:\n",
        "\n",
        "        conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "\n",
        "        public_url = ngrok.connect(5000)\n",
        "\n",
        "        print(f\"Backend is running and accessible at: {public_url}\")\n",
        "\n",
        "        app.run(port=5000, host='0.0.0.0')\n",
        "    except Exception as e:\n",
        "\n",
        "        print(f\"Ngrok failed to start. Check your NGROK_AUTH_TOKEN. Error: {e}\")\n",
        "\n",
        "        print(\"Attempting to run Flask server locally on port 5000 without ngrok.\")\n",
        "        app.run(port=5000, host='0.0.0.0')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}